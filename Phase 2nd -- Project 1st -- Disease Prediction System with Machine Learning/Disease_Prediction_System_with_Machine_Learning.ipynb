{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Data Collection\n",
        "\n",
        "You need to gather a diverse dataset containing relevant health features. For this example, we'll use a public dataset, such as the Heart Disease dataset from the UCI Machine Learning Repository."
      ],
      "metadata": {
        "id": "7Hf3SRQYHFd1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF5tWH9gG43Q",
        "outputId": "d6132950-abbd-4aa2-8622-e2a2305640ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
            "0  63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n",
            "1  67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n",
            "2  67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n",
            "3  37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n",
            "4  41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n",
            "\n",
            "   slope   ca thal  target  \n",
            "0    3.0  0.0  6.0       0  \n",
            "1    2.0  3.0  3.0       2  \n",
            "2    2.0  2.0  7.0       1  \n",
            "3    3.0  0.0  3.0       0  \n",
            "4    1.0  0.0  3.0       0  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (replace the URL with the path to your dataset)\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
        "columns = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"]\n",
        "data = pd.read_csv(url, names=columns)\n",
        "\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Data Preprocessing\n",
        "\n",
        "1) Handle missing values.\n",
        "\n",
        "2) Normalize or standardize features.\n",
        "\n",
        "3) Convert categorical variables into numerical ones if necessary."
      ],
      "metadata": {
        "id": "mB-UUcxQHTCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Handling missing values\n",
        "data = data.replace('?', np.nan)\n",
        "data = data.dropna()\n",
        "\n",
        "# Convert data types\n",
        "data = data.apply(pd.to_numeric)\n",
        "\n",
        "# Normalize the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data.drop('target', axis=1))\n",
        "\n",
        "# Prepare the final dataset\n",
        "X = pd.DataFrame(data_scaled, columns=data.columns[:-1])\n",
        "y = data['target']\n",
        "\n",
        "print(X.head())\n",
        "print(y.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uo08uEpZHdxk",
        "outputId": "5bbe7da8-cc1f-4333-a9c5-d6ebd62e54bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        age       sex        cp  trestbps      chol       fbs   restecg  \\\n",
            "0  0.936181  0.691095 -2.240629  0.750380 -0.276443  2.430427  1.010199   \n",
            "1  1.378929  0.691095  0.873880  1.596266  0.744555 -0.411450  1.010199   \n",
            "2  1.378929  0.691095  0.873880 -0.659431 -0.353500 -0.411450  1.010199   \n",
            "3 -1.941680  0.691095 -0.164289 -0.095506  0.051047 -0.411450 -1.003419   \n",
            "4 -1.498933 -1.446980 -1.202459 -0.095506 -0.835103 -0.411450  1.010199   \n",
            "\n",
            "    thalach     exang   oldpeak     slope        ca      thal  \n",
            "0  0.017494 -0.696419  1.068965  2.264145 -0.721976  0.655877  \n",
            "1 -1.816334  1.435916  0.381773  0.643781  2.478425 -0.894220  \n",
            "2 -0.899420  1.435916  1.326662  0.643781  1.411625  1.172577  \n",
            "3  1.633010 -0.696419  2.099753  2.264145 -0.721976 -0.894220  \n",
            "4  0.978071 -0.696419  0.295874 -0.976583 -0.721976 -0.894220  \n",
            "0    0\n",
            "1    2\n",
            "2    1\n",
            "3    0\n",
            "4    0\n",
            "Name: target, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Feature Selection\n",
        "\n",
        "Using feature selection techniques to identify the most influential variables."
      ],
      "metadata": {
        "id": "9YGaDfEHHhDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Select the top 10 features\n",
        "selector = SelectKBest(score_func=f_classif, k=10)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# Get the selected feature names\n",
        "selected_features = X.columns[selector.get_support()]\n",
        "print(\"Selected features:\", selected_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jC5nQnZ8Hv44",
        "outputId": "01b5f426-6770-41f1-dcbe-d4f989ec9d7e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features: Index(['age', 'sex', 'cp', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope',\n",
            "       'ca', 'thal'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Model Development\n",
        "\n",
        "Implement various machine learning algorithms and evaluate their performance."
      ],
      "metadata": {
        "id": "uv283IpPHylX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"SVM\": SVC()\n",
        "}\n",
        "\n",
        "# Train and evaluate the models\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"{name}:\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    # print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "    # print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "    # print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "    # print(\"-\" * 30)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVOLrkK2H3Wz",
        "outputId": "f0421641-7f4f-42de-d5e2-2a98ea9672d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression:\n",
            "Accuracy: 0.6\n",
            "Decision Tree:\n",
            "Accuracy: 0.55\n",
            "Random Forest:\n",
            "Accuracy: 0.6666666666666666\n",
            "SVM:\n",
            "Accuracy: 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Cross-Validation\n",
        "\n",
        "Implement cross-validation techniques to assess the generalization performance of the models."
      ],
      "metadata": {
        "id": "YJy1_vb8H5zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "for name, model in models.items():\n",
        "    scores = cross_val_score(model, X_selected, y, cv=5, scoring='accuracy')\n",
        "    print(f\"{name} Cross-Validation Accuracy: {scores.mean()} ± {scores.std()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqC8gvN_H-X9",
        "outputId": "59def68a-1487-4675-97c7-8a35ed8ff23b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Cross-Validation Accuracy: 0.5854237288135593 ± 0.059743084712457026\n",
            "Decision Tree Cross-Validation Accuracy: 0.4915254237288136 ± 0.04625321284712805\n",
            "Random Forest Cross-Validation Accuracy: 0.6027118644067797 ± 0.024778117794799624\n",
            "SVM Cross-Validation Accuracy: 0.5521468926553672 ± 0.023316022086803587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Hyperparameter Tuning\n",
        "\n",
        "Fine-tune the hyperparameters of selected machine learning models."
      ],
      "metadata": {
        "id": "Rq_yKdTpH_ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Hyperparameter tuning for Random Forest as an example\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters for Random Forest:\", grid_search.best_params_)\n",
        "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYNUV4tuIDAf",
        "outputId": "8780490b-32ab-4f6a-f70e-d88f94cdddab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for Random Forest: {'max_depth': None, 'n_estimators': 100}\n",
            "Best cross-validation accuracy: 0.586613475177305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Model Interpretability (Optional)\n",
        "\n",
        "Enhance the interpretability of the models using SHAP values or feature importance plots."
      ],
      "metadata": {
        "id": "lKeqqlDGIFNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "# SHAP values for Random Forest\n",
        "best_model = grid_search.best_estimator_\n",
        "explainer = shap.TreeExplainer(best_model)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "shap.summary_plot(shap_values[1], X_test, feature_names=selected_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "OxyssBppIPEp",
        "outputId": "3097a581-f2d2-4f0b-8667-5379f5524e18"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'shap'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-330ded5842d0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# SHAP values for Random Forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shap'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: User Interface (Optional)\n",
        "\n",
        "Develop a user-friendly interface using a framework like Flask."
      ],
      "metadata": {
        "id": "DIIltWj3IP32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the trained model\n",
        "model = joblib.load('random_forest_model.pkl')\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    data = request.json\n",
        "    features = [data[feature] for feature in selected_features]\n",
        "    features_scaled = scaler.transform([features])\n",
        "    prediction = model.predict(features_scaled)\n",
        "    return jsonify({'prediction': int(prediction[0])})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "U_nPdEcgIUBL",
        "outputId": "69017ae9-0bed-4570-c0f1-3c8e56121e71"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'random_forest_model.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-3ed6ae381a1e>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'random_forest_model.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethods\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'POST'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'random_forest_model.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9: Integration with Electronic Health Records (EHR) (Optional)\n",
        "\n",
        "Explore the integration with EHR systems to facilitate seamless information flow."
      ],
      "metadata": {
        "id": "CYbsxpYnIWse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 10: Documentation (Optional)\n",
        "\n",
        "Provide comprehensive documentation covering data sources, methodology, model architecture, and usage instructions."
      ],
      "metadata": {
        "id": "X3u0B22zIap-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 11: Validation and Testing\n",
        "\n",
        "Conduct extensive testing and validation to ensure the accuracy, reliability, and robustness of the system."
      ],
      "metadata": {
        "id": "9majrswYIeCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming validation is already performed through cross-validation\n",
        "# Further testing can be done using unseen data if available"
      ],
      "metadata": {
        "id": "kBof2TTPIjKC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}